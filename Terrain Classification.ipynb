{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "amERiJKhLFnK"
      },
      "source": [
        "# Installing Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import model_from_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiB5NIVkAta"
      },
      "source": [
        "Data Preprocessing APIs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCJ9YHgFj9Sq"
      },
      "source": [
        "def get_raw_data(xfile, yfile):\n",
        "    \"\"\"Takes input x and y files and matches the dimensions by extrapolating y data\n",
        "    \n",
        "    This API reads the provided data csv files and creates an output dataframe that is of\n",
        "    size = 4 * dimension(y). The extra data instances at the end of the x files are discarded.\n",
        "     \n",
        "    Arguments:\n",
        "    1. x_files: x sensor data csv\n",
        "    2. y_files: y label corresponding to each data csv\n",
        "    \"\"\"\n",
        "    check_df_y = pd.read_csv(yfile)\n",
        "    check_df_x = pd.read_csv(xfile)\n",
        "    \n",
        "    new_labels = []\n",
        "    for i in check_df_y.iterrows():\n",
        "        new_labels += [i[1][0]] * 4\n",
        "    \n",
        "    new_labels = pd.DataFrame(new_labels)\n",
        "    diff = check_df_x.shape[0] - new_labels.shape[0]\n",
        "    check_df_x = check_df_x.iloc[:-diff,:]\n",
        "    \n",
        "    return check_df_x, new_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqzFhsbYLiX-"
      },
      "source": [
        "def compose_series_data(x_files, y_files, time_steps = 40, step = 1):\n",
        "    \"\"\"Takes input as x and y files and creates windowed training instances suitable for BiLSTM model\n",
        "    \n",
        "    This API takes in x and y csv files, creates raw data with equal x and y dimensions. Then it\n",
        "    creates windows from the x and y data. \n",
        "    \n",
        "    Arguments:\n",
        "    1. x_files: x sensor data csv\n",
        "    2. y_files: y label corresponding to each data csv\n",
        "    3. time_steps: defines the window size. maximum labels (mode) for a window is kept as the\n",
        "        output label for that windowed training instance\n",
        "    4. step: defines the instances to skip before starting the next window\n",
        "    \n",
        "    Returns:\n",
        "    1. stacked windowed training instances from all the files \n",
        "        (Dimension = [total-windows] X [time-steps] X [6])\n",
        "    2. stacked y labels for the corresponding above windowed training instances\n",
        "        (Dimension = [total-windows] X [1])\n",
        "    \"\"\"\n",
        "    \n",
        "    all_x = []\n",
        "    all_y = []\n",
        "    # create a series from all the subjects\n",
        "    for i in range(len(y_files)):\n",
        "        # get x-y balanced data, basically removed the extra values in x\n",
        "        X, y = get_raw_data(x_files[i], y_files[i])\n",
        "        \n",
        "        # scale data\n",
        "        X = scale_data(X, list(X.columns.values))\n",
        "        # create series data for this file\n",
        "        X, y = create_dataset_with_mode_labels(X, y, time_steps = time_steps, step = step)\n",
        "        \n",
        "        all_x.append(X)\n",
        "        all_y.append(y)\n",
        "    return np.concatenate(all_x), np.concatenate(all_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg04fKrWLjk_"
      },
      "source": [
        "def scale_data(df_data, scale_columns):\n",
        "    \"\"\"Scale the input data to make it robust to variation\n",
        "    \n",
        "    This API takes in the raw input data and scales it using RobustScaler\n",
        "    \"\"\"\n",
        "    scaler = RobustScaler()\n",
        "    scaler = scaler.fit(df_data[scale_columns])\n",
        "\n",
        "    df_data.loc[:, scale_columns] = scaler.transform(df_data[scale_columns].to_numpy())\n",
        "    return df_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAwRHp8ILqUs"
      },
      "source": [
        "def create_dataset_with_mode_labels(X, y, time_steps=1, step=1):\n",
        "    \"\"\"Takes in X and y data and creates windows of time-series\n",
        "    \"\"\"\n",
        "    Xs, ys = [], []\n",
        "    for i in range(0, len(X) - time_steps, step):\n",
        "        v = X.iloc[i:(i + time_steps)].values\n",
        "        labels = y.iloc[i: i + time_steps]\n",
        "        Xs.append(v)\n",
        "        ys.append(stats.mode(labels)[0][0])\n",
        "    return np.array(Xs), np.array(ys).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ1iXs6ULrxY"
      },
      "source": [
        "def create_dataset(X, label, time_steps = 40, step = 1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(0, len(X) - time_steps, step):\n",
        "        v = X.iloc[i:(i + time_steps)].values\n",
        "        Xs.append(v)        \n",
        "        ys.append(label)\n",
        "    return np.array(Xs), np.array(ys).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGYEgnd8kQVF"
      },
      "source": [
        "Divide the input files into train, validate and test sets\n",
        "\n",
        "Create windowed time-series input instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJeYsNE9LvlJ"
      },
      "source": [
        "all_y_files = sorted(glob.glob('Data/TrainingData/subject_???_??__y.csv'))\n",
        "all_x_files = sorted(glob.glob('Data/TrainingData/subject_???_??__x.csv'))\n",
        "\n",
        "test_y_files = [\"Data/TrainingData/subject_002_03__y.csv\", \"Data/TrainingData/subject_001_04__y.csv\"]\n",
        "test_x_files = [\"Data/TrainingData/subject_002_03__x.csv\", \"Data/TrainingData/subject_001_04__x.csv\"]\n",
        "\n",
        "val_y_files = [\"Data/TrainingData/subject_003_02__y.csv\", \"Data/TrainingData/subject_007_04__y.csv\"]\n",
        "val_x_files = [\"Data/TrainingData/subject_003_02__x.csv\", \"Data/TrainingData/subject_007_04__x.csv\"]\n",
        "\n",
        "# subtract the files that are in test and val\n",
        "train_y_files = sorted(list(set(all_y_files) ^ set(test_y_files + val_y_files)))\n",
        "train_x_files = sorted(list(set(all_x_files) ^ set(test_x_files + val_x_files)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBwY277EkXK6"
      },
      "source": [
        "TIME_STEPS = 60\n",
        "STEP = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HE5tLJrkaEc"
      },
      "source": [
        "X_train, y_train = compose_series_data(train_x_files, train_y_files, time_steps = TIME_STEPS, step = STEP)\n",
        "X_val, y_val = compose_series_data(val_x_files, val_y_files, time_steps = TIME_STEPS, step = STEP)\n",
        "X_test, y_test = compose_series_data(test_x_files, test_y_files, time_steps = TIME_STEPS, step = STEP)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-6VJcXRkfZ3"
      },
      "source": [
        "Find out class weights since the input data is not balanced.\n",
        "\n",
        "This information is then used by the model while training which samples the data more from the under-represented classes. This way, we perform implicit data balancing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGiY5PiBkiYT"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(y_train),\n",
        "                                                 y_train.ravel())\n",
        "print(class_weights)\n",
        "class_weights = {i:class_weights[i] for i in range(len(class_weights))}\n",
        "class_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuqjeDgXknxQ"
      },
      "source": [
        "Convert input labels to one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEK34eTbklbj"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "enc = enc.fit(y_train)\n",
        "\n",
        "y_train = enc.transform(y_train)\n",
        "y_test = enc.transform(y_test)\n",
        "y_val = enc.transform(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2vfpBpqktGX"
      },
      "source": [
        "Create and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFF-7xbTkqjF"
      },
      "source": [
        "# save the model\n",
        "# note the time steps\n",
        "model_save_dir = \"final_models\"\n",
        "model_name = \"model_BiLSTM_ts%d_s%d\"%(TIME_STEPS, STEP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2dNRkA1kwyX"
      },
      "source": [
        "## Model architecture\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(\n",
        "    keras.layers.Bidirectional(\n",
        "      keras.layers.LSTM(\n",
        "          units=128, \n",
        "          input_shape=[X_train.shape[1], X_train.shape[2]]\n",
        "      )\n",
        "    )\n",
        ")\n",
        "model.add(keras.layers.Dropout(rate=0.5))\n",
        "model.add(keras.layers.Dense(units=128, activation='relu'))\n",
        "model.add(keras.layers.Dense(y_train.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDwOwVIRk1GK"
      },
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_data=(X_val,y_val),\n",
        "    class_weight=class_weights,\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKoVHf34k2B7"
      },
      "source": [
        "## Plots for Training and Validation\n",
        "\n",
        "fig, axs = plt.subplots(2)\n",
        "fig.suptitle('Training and Validation Loss/Accuracy')\n",
        "axs[0].plot(history.history['loss'], label='train_loss')\n",
        "axs[0].plot(history.history['val_loss'], label='val_loss')\n",
        "\n",
        "axs[1].plot(history.history['acc'], label='train_acc')\n",
        "axs[1].plot(history.history['val_acc'], label='val_acc')\n",
        "\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "fig.set_size_inches(8,6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vQgqRlUk65y"
      },
      "source": [
        "Evaluate the model on the test set and checkout final metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S82PviBEk7py"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKDfO-7Nk93n"
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkc_GuHRlDPy"
      },
      "source": [
        "Save the model for future"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtbgda-Vk_yY"
      },
      "source": [
        "def save_model(model, name):\n",
        "    model_json = model.to_json()\n",
        "    with open(\"%s.json\"%(name), \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    model.save_weights(\"%s.hdf5\"%(name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLgjspqQlIvI"
      },
      "source": [
        "def get_model(name):\n",
        "    json_file = open('%s.json'%(name), 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "    loaded_model.load_weights(\"%s.hdf5\"%(name))\n",
        "    model = loaded_model\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaRRXEJ2lJn0"
      },
      "source": [
        "save_model(model, model_save_dir + \"/\" + model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuTIIpjYlMfM"
      },
      "source": [
        "Generate the output for leaderboard test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyI_KbW7lOmk"
      },
      "source": [
        "files=sorted(glob.glob('Data/TestData/subject_???_??__x.csv'))\n",
        "files\n",
        "\n",
        "y_files=sorted(glob.glob('Data/TestData/subject_???_??__y_time.csv'))\n",
        "print(files,y_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izfgYNm3lQlU"
      },
      "source": [
        "def majority_vote(y):\n",
        "    y_out = []\n",
        "    for i in range(0, y.shape[0], 4):\n",
        "        a = list(y[i:i+4])\n",
        "        y_out.append(max(a, key=a.count))\n",
        "    return np.array(y_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQTyc95klSt-"
      },
      "source": [
        "predictions_dir = \"./\"\n",
        "TEST_TIME_STEPS = TIME_STEPS\n",
        "TEST_STEP = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqCCOzeZlW34"
      },
      "source": [
        "to_save_filenames = [\"subject_009_01__y_prediction.csv\", \"subject_010_01__y_prediction.csv\", \"subject_011_01__y_prediction.csv\", \"subject_012_01__y_prediction.csv\"]\n",
        "\n",
        "for i in range(len(files)):\n",
        "    # read x values\n",
        "    df = pd.read_csv(files[i],header=None)\n",
        "    # scale x values\n",
        "    df = scale_data(df, list(df.columns.values))\n",
        "    # read y values\n",
        "    y_df = pd.read_csv(y_files[i],header=None)\n",
        "    print(y_df.shape)\n",
        "    \n",
        "    # pad x values according to the expected y values\n",
        "    to_add = y_df.shape[0] * 4 - df.shape[0] + TIME_STEPS\n",
        "    to_add_df = pd.DataFrame(df.iloc[-to_add:])\n",
        "    df = df.append(to_add_df)\n",
        "    \n",
        "    # create windowed dataset\n",
        "    X_test, y_test = create_dataset(df, 0, TEST_TIME_STEPS, TEST_STEP)\n",
        "    \n",
        "    # run model\n",
        "    y_test = model.predict(X_test)\n",
        "    \n",
        "    # convert one-hot to integer class labels\n",
        "    y_test_int = np.argmax(y_test, axis=1)\n",
        "    \n",
        "    # reduce the class size by a factor of 4 by taking majority vote\n",
        "    # from a window of 4 elements\n",
        "    y_actual = majority_vote(y_test_int)\n",
        "    \n",
        "    print(y_actual.size)\n",
        "    \n",
        "    # convert into series to export to csv\n",
        "    y_series = pd.Series(y_actual)\n",
        "    y_series.to_csv(predictions_dir + \"/\" + to_save_filenames[i])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}